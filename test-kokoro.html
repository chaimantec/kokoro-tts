<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kokoro.js Test</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }
    .container {
      display: flex;
      flex-direction: column;
      gap: 20px;
    }
    textarea {
      width: 100%;
      height: 100px;
      padding: 10px;
      box-sizing: border-box;
    }
    button {
      padding: 10px 15px;
      background-color: #4CAF50;
      color: white;
      border: none;
      cursor: pointer;
      width: 200px;
    }
    button:disabled {
      background-color: #cccccc;
      cursor: not-allowed;
    }
    .status {
      padding: 10px;
      border: 1px solid #ddd;
      background-color: #f9f9f9;
      min-height: 100px;
    }
    .log {
      white-space: pre-wrap;
      font-family: monospace;
      height: 200px;
      overflow-y: auto;
      padding: 10px;
      background-color: #f0f0f0;
      border: 1px solid #ddd;
    }
  </style>
</head>
<body>
  <h1>Kokoro.js Test</h1>

  <div class="container">
    <div>
      <h2>Status</h2>
      <div class="status" id="status">Not initialized</div>
    </div>

    <div>
      <h2>Text to Speech</h2>
      <textarea id="textInput" placeholder="Enter text to convert to speech...">This is a test of the Kokoro text-to-speech system.</textarea>
      <div style="margin-top: 10px;">
        <div style="margin-bottom: 10px;">
          <label>
            <input type="checkbox" id="useWebGPU"> Try to use WebGPU (faster on supported devices)
          </label>
        </div>
        <button id="loadButton">Load Model</button>
        <button id="speakButton" disabled>Generate Speech</button>
        <button id="playButton" disabled>Play Audio</button>
      </div>
    </div>

    <div>
      <h2>Log</h2>
      <div class="log" id="log"></div>
    </div>
  </div>

  <!-- Import kokoro-js from node_modules -->
  <script type="module">
    import { KokoroTTS, TextSplitterStream } from '/node_modules/kokoro-js/dist/kokoro.js';

    // Elements
    const statusEl = document.getElementById('status');
    const textInputEl = document.getElementById('textInput');
    const useWebGPUEl = document.getElementById('useWebGPU');
    const loadButtonEl = document.getElementById('loadButton');
    const speakButtonEl = document.getElementById('speakButton');
    const playButtonEl = document.getElementById('playButton');
    const logEl = document.getElementById('log');

    // Log function
    function log(message) {
      console.log(message);
      logEl.textContent += message + '\n';
      logEl.scrollTop = logEl.scrollHeight;
    }

    // Global variables
    let kokoro = null;
    let audioContext = null;
    let audioBuffer = null;
    let audioQueue = [];
    let isPlaying = false;

    // Initialize audio context
    function initAudioContext() {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        log('Audio context initialized');
      } catch (error) {
        log('Error initializing audio context: ' + error.message);
      }
    }

    // Function to play audio chunks from the queue
    async function playNextInQueue() {
      if (audioQueue.length === 0 || isPlaying) {
        return;
      }

      isPlaying = true;
      const audioChunk = audioQueue.shift();

      try {
        log(`Playing chunk with length: ${audioChunk.audio.length}, sampling rate: ${audioChunk.sampling_rate}`);

        // Create audio buffer from Float32Array
        const buffer = audioContext.createBuffer(1, audioChunk.audio.length, audioChunk.sampling_rate);
        const channelData = buffer.getChannelData(0);
        channelData.set(audioChunk.audio);

        // Play the audio
        const source = audioContext.createBufferSource();
        source.buffer = buffer;
        source.connect(audioContext.destination);

        // When this chunk ends, play the next one
        source.onended = () => {
          log('Chunk playback completed');
          isPlaying = false;
          playNextInQueue();
        };

        source.start();
        log('Playing audio chunk...');
      } catch (error) {
        log('Error playing audio chunk: ' + error.message);
        isPlaying = false;
        playNextInQueue(); // Try the next chunk
      }
    }

    // Load the model
    loadButtonEl.addEventListener('click', async () => {
      try {
        loadButtonEl.disabled = true;
        statusEl.textContent = 'Loading model...';
        log('Initializing Kokoro...');

        // Check if WebGPU is requested
        const useWebGPU = useWebGPUEl.checked;

        if (useWebGPU) {
          log('Attempting to use WebGPU for model inference...');

          // Check if WebGPU is available
          if (navigator.gpu) {
            log('WebGPU is supported in this browser');

            try {
              // Initialize Kokoro with WebGPU
              log('Loading model with WebGPU...');
              kokoro = await KokoroTTS.from_pretrained('onnx-community/Kokoro-82M-v1.0-ONNX', {
                dtype: 'fp32',
                device: 'webgpu'
              });
              log('Model loaded successfully with WebGPU!');
            } catch (gpuError) {
              log(`WebGPU initialization failed: ${gpuError.message}`);
              log('Falling back to default backend...');

              // Fall back to default backend
              kokoro = await KokoroTTS.from_pretrained('onnx-community/Kokoro-82M-v1.0-ONNX', {
                dtype: 'q8',
                device: 'wasm'
              });
            } 
          } else {
            log('WebGPU is not supported in this browser');
            log('Using default backend instead');

            // Use default backend
            kokoro = await KokoroTTS.from_pretrained('onnx-community/Kokoro-82M-v1.0-ONNX', {
              dtype: 'q8',
              device: 'wasm'
            });
          }
        } else {
          // Initialize Kokoro with default backend
          log('Loading model with default backend...');
          kokoro = await KokoroTTS.from_pretrained('onnx-community/Kokoro-82M-v1.0-ONNX', {
            dtype: 'q8',
            device: 'wasm'
          });
        }

        log('Model loaded successfully!');
        statusEl.textContent = 'Model loaded successfully!';
        speakButtonEl.disabled = false;

        // Initialize audio context
        initAudioContext();
      } catch (error) {
        log('Error loading model: ' + error.message);
        statusEl.textContent = 'Error loading model: ' + error.message;
        loadButtonEl.disabled = false;
      }
    });

    // Generate speech with streaming
    speakButtonEl.addEventListener('click', async () => {
      try {
        const text = textInputEl.value.trim();
        if (!text) {
          log('Please enter some text');
          return;
        }

        // Clear any existing audio queue
        audioQueue = [];

        // Reset UI state
        speakButtonEl.disabled = true;
        playButtonEl.disabled = true;
        statusEl.textContent = 'Generating speech...';
        log(`Generating speech for: "${text}"`);

        // Use the TextSplitterStream for proper sentence splitting
        const splitter = new TextSplitterStream();

        // Split the text into sentences and push them to the stream
        splitter.push(text);

        // Close the stream to indicate no more text will be added
        splitter.close();

        // Use the stream method with the TextSplitterStream
        let sentenceCount = 0;
        let totalAudioLength = 0;

        // Start streaming generation
        for await (const chunk of kokoro.stream(splitter)) {
          sentenceCount++;
          log(`Received chunk ${sentenceCount}: "${chunk.text}"`);
          log(`Phonemes: "${chunk.phonemes}"`);

          // Add the audio chunk to the queue
          audioQueue.push(chunk.audio);
          totalAudioLength += chunk.audio.audio.length;

          // Log detailed information about the audio chunk
          log(`Audio chunk ${sentenceCount}: length=${chunk.audio.audio.length}, sampling_rate=${chunk.audio.sampling_rate}`);

          // Start playing if this is the first chunk and not already playing
          if (sentenceCount === 1) {
            playNextInQueue();
          }
        }

        log('Speech generation completed!');
        log(`Generated ${sentenceCount} chunks with total length: ${totalAudioLength} samples`);
        statusEl.textContent = 'Speech generation completed!';

        // Store the last complete audio for the "Play" button
        if (sentenceCount > 0) {
          // Create a new array to hold all the audio chunks
          const allChunks = [];

          // Calculate total length
          let totalLength = 0;
          for (let i = 0; i < sentenceCount; i++) {
            if (i < audioQueue.length) {
              // For chunks still in the queue
              totalLength += audioQueue[i].audio.length;
              allChunks.push(audioQueue[i]);
            }
          }

          // Combine all audio chunks into one buffer for the play button
          audioBuffer = {
            audio: new Float32Array(totalLength),
            sampling_rate: allChunks[0]?.sampling_rate || 24000
          };

          let offset = 0;
          for (const chunk of allChunks) {
            audioBuffer.audio.set(chunk.audio, offset);
            offset += chunk.audio.length;
          }

          log(`Created combined audio buffer with length: ${audioBuffer.audio.length}`);
        }

        speakButtonEl.disabled = false;
        playButtonEl.disabled = false;
      } catch (error) {
        log('Error generating speech: ' + error.message);
        statusEl.textContent = 'Error generating speech: ' + error.message;
        speakButtonEl.disabled = false;
      }
    });

    // Play audio (full buffer)
    playButtonEl.addEventListener('click', async () => {
      try {
        if (!audioBuffer) {
          log('No audio to play');
          return;
        }

        statusEl.textContent = 'Playing complete audio...';
        log('Playing complete audio...');

        // Create audio buffer from Float32Array
        const buffer = audioContext.createBuffer(1, audioBuffer.audio.length, audioBuffer.sampling_rate);
        const channelData = buffer.getChannelData(0);
        channelData.set(audioBuffer.audio);

        // Play the audio
        const source = audioContext.createBufferSource();
        source.buffer = buffer;
        source.connect(audioContext.destination);
        source.start();

        source.onended = () => {
          log('Complete audio playback finished');
          statusEl.textContent = 'Complete audio playback finished';
        };
      } catch (error) {
        log('Error playing audio: ' + error.message);
        statusEl.textContent = 'Error playing audio: ' + error.message;
      }
    });

    // Initial log
    log('Kokoro.js test page loaded');

    // Check WebGPU availability
    if (navigator.gpu) {
      log('WebGPU is supported in this browser');

      // Set WebGPU checkbox to checked by default if available
      useWebGPUEl.checked = true;
    } else {
      log('WebGPU is not supported in this browser');
      log('Using default backend (WebGL or WASM) will be slower');

      // Disable WebGPU checkbox if not available
      useWebGPUEl.disabled = true;
      useWebGPUEl.checked = false;
      useWebGPUEl.parentElement.style.opacity = '0.5';
      useWebGPUEl.parentElement.title = 'WebGPU is not supported in this browser';
    }
  </script>
</body>
</html>
